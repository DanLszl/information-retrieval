{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment - Part B\n",
    "\n",
    "\n",
    "  ###### Dániel László(12305782)     Kishan Parshotam(11635959)      Leonardo Romor(12261734)    \n",
    "  \n",
    "* In this notebook we implement the experiment to determine an adequate sample size when testing a new experimental (E) information retrieval system. \n",
    "\n",
    "This notebook is split into the following sections:\n",
    "* Offline\n",
    "* Interleaving Methods\n",
    "* Estimating Click Models\n",
    "* Experiments\n",
    "* Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All imports are here\n",
    "from collections import defaultdict\n",
    "from collections import namedtuple\n",
    "from itertools import permutations\n",
    "import random\n",
    "import string\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import binomial\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt \n",
    "from math import sqrt, ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline\n",
    "\n",
    "We start by generating all possible ranking scenarios, and assigning these to their respective buckets. For this we need to first estimate ERR's. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERR Calculation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_prob_relevance(grade, max_grade=1):\n",
    "    return (2 ** grade - 1) / (2 ** max_grade)\n",
    "\n",
    "\n",
    "def calculate_err(ranking_list):\n",
    "    err = 0\n",
    "    for r_index, rank in enumerate(ranking_list):\n",
    "        i_prod = 1\n",
    "        for i_index in range(r_index):\n",
    "            i_prod *= (1 - calculate_prob_relevance(ranking_list[i_index]))\n",
    "        err += 1 / (r_index + 1) * i_prod * calculate_prob_relevance(rank)\n",
    "\n",
    "    return err\n",
    "\n",
    "\n",
    "def calculate_delta_err(pair):\n",
    "    return calculate_err(pair[0]) - calculate_err(pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating ranking pairs\n",
    "* Create all possible combinations of relevance for two systems, E and P. \n",
    "* Calculate the $\\Delta$ERR for every pair.\n",
    "* Keep only pairs with positive $\\Delta$ERR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_positive_pairs(num_labels=2):\n",
    "    labels = [label for label in range(num_labels)]\n",
    "    ranks = [(x, y, z) for x in labels for y in labels for z in labels]\n",
    "\n",
    "    return [(e, p, calculate_delta_err((e, p)))\n",
    "            for e in ranks\n",
    "            for p in ranks\n",
    "            if calculate_delta_err((e, p)) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating all possible rankings for each of the relevance-pairs\n",
    "* Now that all possible combinations of ranking have been made, we can generate fake document id's.\n",
    "* For this we can fix the ranking of one of the systems, and then check all possible scenarios for the other. \n",
    "* The filter function only keeps rankings that are unique in their overlappings.\n",
    "    * For example: (P=[1,2,3]; E=[4,5,6]) and (P=[1,2,3]; E=[5,6,7]) are the same as there are no overlaps in either.\n",
    "    * If there are any overlapping documents, they must have the same relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RankingPair = namedtuple('RankingPair', ['E', 'P', 'rel_E', 'rel_P'])\n",
    "\n",
    "class RankingPairs:\n",
    "    def __init__(self, k):\n",
    "        ''' k: cutoff '''\n",
    "        self.ranking1 = tuple(range(k))\n",
    "        all_ids = list(range(2*k))\n",
    "        self.rankings2 = list(permutations(all_ids, k))\n",
    "        self.k = k\n",
    "        self.masks = []\n",
    "\n",
    "    def filter(self, ranking1, ranking2, relevances1, relevances2):\n",
    "        # based on the overlaps, mask will have a value, which we compare with previous masks.\n",
    "        # We only keep the current rankings, if the mask was not seen before.\n",
    "        # -1 signals no overlap.\n",
    "        # E.g.: [2, -1, -1] means that the second system's first document\n",
    "        #                   overlaps with the first system's second document.\n",
    "        mask = []\n",
    "        for doc1, rel1 in zip(ranking1, relevances1):\n",
    "            for doc2, rel2 in zip(ranking2, relevances2):\n",
    "                if doc1 == doc2 and rel1 != rel2:\n",
    "                    return False\n",
    "                elif doc1 == doc2:\n",
    "                    mask.append(doc1)\n",
    "                else:\n",
    "                    mask.append(-1)\n",
    "\n",
    "        if mask in self.masks:\n",
    "            return False\n",
    "        else:\n",
    "            self.masks.append(mask)\n",
    "            return True\n",
    "\n",
    "    def generate_valid_rankings(self, relevance_pair):\n",
    "        self.masks = []\n",
    "        rel_B, rel_A = relevance_pair\n",
    "\n",
    "        if len(rel_A) != self.k or len(rel_B) != self.k:\n",
    "            raise ValueError(\n",
    "                'The relevance lists should have a length of {}'.format(self.k))\n",
    "        else:\n",
    "            ranking1 = self.ranking1\n",
    "            valid = [RankingPair(ranking2, ranking1, rel_B, rel_A)\n",
    "                     for ranking2 in self.rankings2\n",
    "                     if self.filter(ranking1, ranking2, rel_A, rel_B)]\n",
    "            return valid\n",
    "\n",
    "\n",
    "def convert_pairs_of_relevances_to_possible_rankings(pairs):\n",
    "    k = len(pairs[0][0])\n",
    "    ranking_pairs = RankingPairs(k)\n",
    "    result = {tuple(pair): {'rankings': ranking_pairs.generate_valid_rankings(\n",
    "        pair), 'd_ERR': d_ERR} for *pair, d_ERR in pairs}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating buckets\n",
    "* We use the buckets to group the ranking pairs and doc ids.\n",
    "* Each bucket corresponds to a row in the tables shown in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_buckets(n_buckets=10, offset=0.05):\n",
    "    buckets = dict()\n",
    "    for bucket_id in range(n_buckets):\n",
    "        buckets[bucket_id] = dict()\n",
    "        buckets[bucket_id]['range'] = [round(1/10*bucket_id, 2), round(1/n_buckets*(bucket_id + 1), 2)]\n",
    "\n",
    "    # offset first and last bucket\n",
    "    buckets[0]['range'][0] = offset\n",
    "    buckets[n_buckets - 1]['range'][1] -= offset\n",
    "\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocating  relevances to buckets\n",
    "* We take all relevances, and their possible rankings, and allocate them to buckets based on their $\\Delta$ERR.\n",
    "* We only include buckets, that have at least one relevance pairs, deleting redundant ones. Specifically, bucket 8, 9 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def allocate_to_buckets(relevances, buckets):\n",
    "    '''\n",
    "        relevances: Dict[int, RelevancePair]\n",
    "        buckets: A Dict created by create_buckets()\n",
    "    '''\n",
    "    \n",
    "    for bucket in buckets.values():\n",
    "        bucket['relevances'] = {}\n",
    "\n",
    "    for rel_pair, data in relevances.items():\n",
    "        delta_err = data['d_ERR']\n",
    "        for bucket in buckets.values():\n",
    "            interval = bucket['range']\n",
    "            if interval[0] <= delta_err < interval[1]:\n",
    "                bucket['relevances'][rel_pair] = data\n",
    "                break\n",
    "    \n",
    "    return delete_empty(buckets)\n",
    "\n",
    "\n",
    "def delete_empty(buckets):\n",
    "    buckets_to_delete = []\n",
    "    for b_id in buckets.keys():\n",
    "        if not buckets[b_id]['relevances']:\n",
    "            buckets_to_delete.append(b_id)\n",
    "    for i in buckets_to_delete:\n",
    "        del buckets[i]\n",
    "                \n",
    "    return buckets\n",
    "\n",
    "\n",
    "def get_buckets():\n",
    "    pairs = create_positive_pairs()\n",
    "    rankings = convert_pairs_of_relevances_to_possible_rankings(pairs)\n",
    "    buckets = create_buckets()\n",
    "    allocate_to_buckets(rankings, buckets)\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test - prints all possible doc_id's combinations for every relevance ranking pair for each bucket. \n",
    "pairs = create_positive_pairs()\n",
    "rankings = convert_pairs_of_relevances_to_possible_rankings(pairs)\n",
    "buckets = create_buckets()\n",
    "allocate_to_buckets(rankings, buckets)\n",
    "pprint(buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interleaving Methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team-Draft Interleaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randbit():\n",
    "    return random.random() > 0.5\n",
    "\n",
    "\n",
    "class TeamDraftInterleaving:\n",
    "    def __init__(self, production_ranking, experimental_ranking, k=3):\n",
    "        self.prod_ranking = list(production_ranking)\n",
    "        self.exp_ranking = list(experimental_ranking)\n",
    "        self.k = k   # cutoff\n",
    "\n",
    "    @staticmethod\n",
    "    def choose_from_A_and_delete_from_both(A, B, ranking_name):\n",
    "        '''choose_from_A_and_delete_from_both'''\n",
    "        doc = A.pop(0)\n",
    "        try:\n",
    "            i = B.index(doc)\n",
    "            del B[i]\n",
    "        except ValueError:\n",
    "            pass\n",
    "            # print('{} : doc {} is not in the other ranking'.format(ranking_name, doc))\n",
    "        return doc\n",
    "\n",
    "    def choose_from_prod(self):\n",
    "        return self.choose_from_A_and_delete_from_both(self.prod_ranking, self.exp_ranking, 'Prod')\n",
    "\n",
    "    def choose_from_exp(self):\n",
    "        return self.choose_from_A_and_delete_from_both(self.exp_ranking, self.prod_ranking, 'Exp')\n",
    "\n",
    "    def run(self):\n",
    "        '''\n",
    "            production_ranking = [a_1, a_2, ..., a_n]\n",
    "            experimental_ranking = [b_1, b_2, ..., b_n]\n",
    "            where a_i and b_i are document ids\n",
    "            It is assumed that both rankings contain unique documents, e.g. no duplicates in for the same system\n",
    "        '''\n",
    "\n",
    "        interleaved = []  # Interleaved ranking\n",
    "        TeamA = []\n",
    "        TeamB = []\n",
    "\n",
    "        for interleaved_idx in range(len(self.prod_ranking) + len(self.exp_ranking)):\n",
    "            if len(self.prod_ranking) == 0 and len(self.exp_ranking) == 0:\n",
    "                break\n",
    "            \n",
    "            # We only want the first three documents\n",
    "            if len(interleaved) == self.k:\n",
    "                break\n",
    "\n",
    "            if len(self.exp_ranking) == 0:\n",
    "                doc = self.choose_from_prod()\n",
    "                winning_team = TeamA\n",
    "            elif len(self.prod_ranking) == 0:\n",
    "                doc = self.choose_from_exp()\n",
    "                winning_team = TeamB\n",
    "            elif len(TeamA) < len(TeamB):\n",
    "                # Chosing from production ranking\n",
    "                doc = self.choose_from_prod()\n",
    "                winning_team = TeamA\n",
    "            elif len(TeamB) < len(TeamA):\n",
    "                doc = self.choose_from_exp()\n",
    "                winning_team = TeamB\n",
    "            else:\n",
    "                if randbit():\n",
    "                    doc = self.choose_from_prod()\n",
    "                    winning_team = TeamA\n",
    "                else:\n",
    "                    doc = self.choose_from_exp()\n",
    "                    winning_team = TeamB\n",
    "\n",
    "            interleaved.append(doc)\n",
    "            winning_team.append(doc)\n",
    "\n",
    "        return interleaved, TeamA, TeamB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions for testing\n",
    "def generate_docs():\n",
    "    letters = list(string.ascii_lowercase)\n",
    "    docs = letters[:6]\n",
    "    prod_ranking = random.sample(docs, len(docs))\n",
    "    exp_ranking = random.sample(docs, len(docs))\n",
    "    print('Ranking P:', prod_ranking)\n",
    "    print('Ranking E:', exp_ranking)\n",
    "    return prod_ranking, exp_ranking\n",
    "\n",
    "\n",
    "def test_team_draft_interleaving(docs=None):\n",
    "    docs = generate_docs() if docs is None else docs\n",
    "    prod_ranking, exp_ranking = docs\n",
    "    team_draft = TeamDraftInterleaving(prod_ranking, exp_ranking)\n",
    "    interleaved_ranking, prod_docs, exp_docs = team_draft.run()\n",
    "    print('Rankings Interleaved:', interleaved_ranking)\n",
    "    print('Docs of P:', prod_docs)\n",
    "    print('Docs of E:', exp_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic interleaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / np.sum(e_x)\n",
    "\n",
    "\n",
    "class ProbabilisticInterleaving:\n",
    "    def __init__(self, prod_ranking, exp_ranking, tau=3, k=3):\n",
    "        self.prod_ranking = list(prod_ranking)\n",
    "        self.prod_ranks = list(range(1, len(prod_ranking)+1))\n",
    "\n",
    "        self.exp_ranking = list(exp_ranking)\n",
    "        self.exp_ranks = list(range(1, len(exp_ranking)+1))\n",
    "        self.tau = tau    # Tau is used according to the paper \"A Probabilistic Method for Inferring Preferences from Clicks\"\n",
    "        self.k = k\n",
    "        \n",
    "        self.update_distributions()\n",
    "\n",
    "        self.interleaved = []\n",
    "\n",
    "    def update_distributions(self):\n",
    "        self.prod_dist = self.probability_dist_over_ranks(self.prod_ranks)\n",
    "        self.exp_dist = self.probability_dist_over_ranks(self.exp_ranks)\n",
    "\n",
    "    def probability_dist_over_ranks(self, ranks):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        ranking : list\n",
    "            A list of documents.\n",
    "        tau : int\n",
    "            The second parameter.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            probability distribution over the given ranking\n",
    "        '''\n",
    "        tau = self.tau\n",
    "        ranks = np.array(ranks)\n",
    "        inverse_ranks = (1 / ranks) ** tau\n",
    "        p_dist = softmax(inverse_ranks)\n",
    "        return p_dist\n",
    "\n",
    "    def choose_from_A(self, A, p_A, ranks_A, B, p_B, ranks_B, A_name):\n",
    "        doc = np.random.choice(A, p=p_A)\n",
    "        i = A.index(doc)\n",
    "        del A[i]\n",
    "        del ranks_A[i]\n",
    "\n",
    "        try:\n",
    "            i = B.index(doc)\n",
    "            del B[i]\n",
    "            del ranks_B[i]\n",
    "        except ValueError:\n",
    "            pass\n",
    "            # print('Doc {} was not in the {} ranking'.format(doc, A_name))\n",
    "\n",
    "        self.update_distributions()\n",
    "        return doc\n",
    "\n",
    "    def choose_from_prod(self):\n",
    "        doc = self.choose_from_A(self.prod_ranking, self.prod_dist, self.prod_ranks,\n",
    "                           self.exp_ranking, self.exp_dist, self.exp_ranks, 'Prod')\n",
    "        return doc\n",
    "\n",
    "    def choose_from_exp(self):\n",
    "        doc = self.choose_from_A(self.exp_ranking, self.exp_dist, self.exp_ranks,\n",
    "                           self.prod_ranking, self.prod_dist, self.prod_ranks, 'Exp')\n",
    "        return doc\n",
    "\n",
    "    def run(self):\n",
    "        '''\n",
    "            production_ranking = [a_1, a_2, ..., a_n]\n",
    "            experimental_ranking = [b_1, b_2, ..., b_n]\n",
    "            where a_i and b_i are document ids\n",
    "            It is assumed that both rankings contain unique documents\n",
    "        '''\n",
    "\n",
    "        interleaved = []  # Interleaved ranking\n",
    "        TeamA = []\n",
    "        TeamB = []\n",
    "\n",
    "        for interleaved_idx in range(len(self.prod_ranking)+len(self.exp_ranking)):\n",
    "            if len(self.prod_ranking) == 0 and len(self.exp_ranking) == 0:\n",
    "                break\n",
    "            \n",
    "            if len(interleaved) == self.k:\n",
    "                break\n",
    "            \n",
    "            if len(self.exp_ranking) == 0:\n",
    "                doc = self.choose_from_prod()\n",
    "                winning_team = TeamA\n",
    "            elif len(self.prod_ranking) == 0:\n",
    "                doc = self.choose_from_exp()\n",
    "                winning_team = TeamB\n",
    "            elif randbit():\n",
    "                doc = self.choose_from_prod()\n",
    "                winning_team = TeamA\n",
    "            else:\n",
    "                doc = self.choose_from_exp()\n",
    "                winning_team = TeamB\n",
    "\n",
    "            interleaved.append(doc)\n",
    "            winning_team.append(doc)\n",
    "\n",
    "        return interleaved, TeamA, TeamB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_probabilistic():\n",
    "    prod_ranking, exp_ranking = generate_docs()\n",
    "    interleaving = ProbabilisticInterleaving(prod_ranking, exp_ranking)\n",
    "\n",
    "    interleaved, prod_docs, exp_docs = interleaving.run()\n",
    "\n",
    "    print('Rankings Interleaved:', interleaved)\n",
    "    print('Docs of P:', prod_docs)\n",
    "    print('Docs of E:', exp_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "print('Team-Draft interleaving test:')\n",
    "test_team_draft_interleaving()\n",
    "print('\\n')\n",
    "print('Proabilistic interleaving test:')\n",
    "test_probabilistic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Click Models\n",
    "\n",
    "The goal in this section is to estimate the parameters of two click models:\n",
    "* Random Click Model\n",
    "* Position-Based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the Yandex Log file\n",
    "\n",
    "In order to estimate the parameters for our click models, we first parse the YandexRelPredChallenge log file. For this we assume that each query is a new session, identified by a unique session_id. Therefore, even repeated queries, under the same SessionID are considered two different session_ids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_log_data(log_file):\n",
    "    '''\n",
    "    Parser for Yandex log file.\n",
    "    Outputs two dictionaries: \n",
    "        - session_data: ranked ListofURLs and queries information\n",
    "        - clicks_per_session: URLs clicked for the session\n",
    "    Outputs are linked via the session_id. \n",
    "    '''\n",
    "    session_data = defaultdict(lambda: defaultdict(list))\n",
    "    clicks_per_session = defaultdict(list)\n",
    "\n",
    "    with open(log_file, 'r') as in_file:\n",
    "        session_id = 0\n",
    "        for line in in_file:\n",
    "            line = line.split()\n",
    "            if line[2] == \"C\":\n",
    "                clicks_per_session[session_id-1].append(line[3])    #clicked query_id\n",
    "            if line[2] == \"Q\":\n",
    "                ranked_docs = line[5:]\n",
    "                rank_count = 0 \n",
    "                session_data[session_id]['query_id'].append(line[3]) #query id\n",
    "                for doc in ranked_docs:\n",
    "                    session_data[session_id]['doc_ids'].append(doc) #doc id\n",
    "                    rank_count += 1\n",
    "                session_id += 1\n",
    "\n",
    "    return session_data, clicks_per_session\n",
    "\n",
    "\n",
    "def get_session_data_and_clicks_per_session():\n",
    "    '''\n",
    "    Calls parse_log_data using YandexRelPredChallenge\n",
    "    '''\n",
    "    log_file = \"YandexRelPredChallenge.txt\"\n",
    "    session_data, clicks_per_session = parse_log_data(log_file)\n",
    "    \n",
    "    return session_data, clicks_per_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Click Model\n",
    "\n",
    "We implement the RCM and define three methods, according to the assignment:\n",
    "* A method that learns the parameters of the model given a set of training data\n",
    "* A method that predicts the click probability given a ranked list of relevance labels\n",
    "* A method that decides - stochastically - whether a document is clicked based on these probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomClickModel:\n",
    "    def __init__(self, session_data, clicks_per_session):\n",
    "        self.rho = self.estimate_parameters(session_data, clicks_per_session)\n",
    "\n",
    "    def estimate_parameters(self, session_data, clicks_per_session):\n",
    "        ndocs = 0\n",
    "        nclicks = 0\n",
    "        for id, docs in session_data.items():\n",
    "            ndocs += len(docs['doc_ids'])\n",
    "            nclicks += len(clicks_per_session[id])\n",
    "        return nclicks / ndocs\n",
    "\n",
    "    def simulate_clicks(self, interleaved_results, *args):\n",
    "        '''\n",
    "        Returns idx of clicked document on the interleaved_results list\n",
    "        '''\n",
    "        p_click = self.click_probability(interleaved_results)\n",
    "        clicks = np.array([binomial(1, prob) for prob in p_click])\n",
    "        return np.where(clicks)[0]\n",
    "        \n",
    "    def click_probability(self, interleaved_results):\n",
    "        click_probabilities = [self.rho for _ in range(len(interleaved_results))]\n",
    "        return click_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "log_file = \"YandexRelPredChallenge.txt\"\n",
    "session_data, clicks_per_session = parse_log_data(log_file)\n",
    "rcm = RandomClickModel(session_data, clicks_per_session)\n",
    "pprint(\"Rho: %0.3f\" % rcm.rho)\n",
    "\n",
    "interleaved_test = [1, 2, 3]\n",
    "print('Click probabilities:', rcm.click_probability(interleaved_test))\n",
    "print('Click indexes:', rcm.simulate_clicks(interleaved_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-Based Model\n",
    "\n",
    "We implement the PBM and define the same three methods, and some helper ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PositionBasedModel:\n",
    "    def __init__(self, session_data, clicks_per_session, fixed_alpha = 1e-2):\n",
    "        self.alphas, self.gammas = self.estimate_parameters(session_data, clicks_per_session)\n",
    "        self.fixed_alpha = fixed_alpha\n",
    "\n",
    "    def estimate_alphas(self, alphas, gammas, session_data, clicks_per_session):\n",
    "        _alphas = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "        for session, _ in session_data.items():\n",
    "            query_id = session_data[session]['query_id'][0]\n",
    "            for doc_rank, doc in enumerate(session_data[session]['doc_ids']):\n",
    "                if doc in clicks_per_session[session]:\n",
    "                    weight = 1\n",
    "                else:\n",
    "                    weight = (1 - gammas[doc_rank])*alphas[(query_id, doc)] / (1 - gammas[doc_rank]*alphas[(query_id, doc)])\n",
    "\n",
    "                _alphas[(query_id, doc)]['weigth'].append(weight)           \n",
    "                _alphas[(query_id, doc)]['count'].append(1)\n",
    "\n",
    "        for pair in _alphas.keys():\n",
    "            sum_weights = sum(_alphas[pair]['weigth']) + 1    # We figured this from pyclick, otherwise gammas converge to 1\n",
    "            sum_count = sum(_alphas[pair]['count']) + 2\n",
    "            alphas[pair] = sum_weights / sum_count\n",
    "\n",
    "        return alphas\n",
    "\n",
    "\n",
    "    def estimate_gammas(self, alphas, gammas, session_data, clicks_per_session):\n",
    "        _gammas = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "        for session, _ in session_data.items():\n",
    "            query_id = session_data[session]['query_id'][0]\n",
    "            for doc_rank, doc in enumerate(session_data[session]['doc_ids']):\n",
    "                if doc in clicks_per_session[session]:\n",
    "                    weight = 1\n",
    "                else:\n",
    "                    weight = (1 - alphas[(query_id, doc)]) * gammas[doc_rank] / (1 - gammas[doc_rank]*alphas[(query_id, doc)])\n",
    "\n",
    "                _gammas[doc_rank]['weigth'].append(weight)\n",
    "                _gammas[doc_rank]['count'].append(1)\n",
    "\n",
    "        for rank in _gammas.keys():\n",
    "            sum_weights = sum(_gammas[rank]['weigth']) + 1\n",
    "            sum_count = sum(_gammas[rank]['count']) + 2\n",
    "            gammas[rank] = sum_weights / sum_count\n",
    "\n",
    "        return gammas\n",
    "\n",
    "\n",
    "    def estimate_parameters(self, session_data, clicks_per_session, num_iterations = 50):\n",
    "        '''\n",
    "        EM algorithm. \n",
    "        We tested and 50 iterations are sufficient. A convergence criteria could be\n",
    "        created too by comparing gammas with new gammas and\n",
    "        taking the difference of the means. \n",
    "        '''\n",
    "        alphas = defaultdict(lambda:np.random.uniform(0.25, 0.75))\n",
    "        gammas = defaultdict(lambda:np.random.uniform(0.25, 0.75))\n",
    "\n",
    "        for iteration in tqdm_notebook(range(num_iterations), desc = 'Training PBM'):   \n",
    "            new_alphas = self.estimate_alphas(alphas, gammas, session_data, clicks_per_session)\n",
    "            new_gammas = self.estimate_gammas(alphas, gammas, session_data, clicks_per_session)     \n",
    "            \n",
    "            alphas = new_alphas\n",
    "            gammas = new_gammas\n",
    "\n",
    "        return alphas, gammas\n",
    "\n",
    "\n",
    "    def simulate_clicks(self, interleaved_results, *args):\n",
    "        '''\n",
    "        Simulate clicks according to click_probability\n",
    "        Returns idx of clicked document on the interleaved_results list\n",
    "        '''\n",
    "        p_click = self.click_probability(interleaved_results, args[0])\n",
    "        clicks = np.array([binomial(1, prob) for prob in p_click])\n",
    "\n",
    "        return np.where(clicks)[0]\n",
    "\n",
    "\n",
    "    def click_probability(self, interleaved_results, *args):\n",
    "        '''\n",
    "        P(Click = 1) = P(Gamma) * P(alpha -> fixed_alpha if relevance = 0 else 1 - fixed_alpha)\n",
    "        '''\n",
    "        p_gammas = np.array([val for val in self.gammas.values()])\n",
    "        p_gammas = p_gammas[:len(interleaved_results)]\n",
    "        relevance = args[0]     \n",
    "        p_alphas = np.array([self.fixed_alpha \n",
    "                            if relevance[idx] == 0\n",
    "                            else 1 - self.fixed_alpha\n",
    "                            for idx in range(len(interleaved_results))])\n",
    "\n",
    "        return p_gammas * p_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test and output gammmas ~2 minutes\n",
    "pbm = PositionBasedModel(session_data, clicks_per_session)\n",
    "\n",
    "for idx, gamma in pbm.gammas.items():\n",
    "    print('Gamma for rank {}: {}'.format(idx + 1, gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "interleaved_results = [2, 0, 3]\n",
    "relevance = [0, 1, 1]\n",
    "click_prob = pbm.click_probability(interleaved_results, relevance)\n",
    "clicks = pbm.simulate_clicks(interleaved_results, relevance)\n",
    "\n",
    "print('Click probabilities', click_prob)\n",
    "print('Clicked indexes', clicks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "In this section we combine all of the code above to run our experiments.\n",
    "Steps:\n",
    "* For each interleaving method and click model and we do the following steps:\n",
    "    * For each bucket:\n",
    "        * Sample doc_id pairs of size group_size by a bootstrap (with replacement)\n",
    "        * For each doc_id pair in sampled pairs:\n",
    "            * Simulate k times:\n",
    "                1. Do an interleaving of the ranking pairs\n",
    "                2. Simulate the clicks\n",
    "                3. Assign the winner based on the clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setup \n",
    "DEBUG = False\n",
    "\n",
    "session_data, clicks_per_session = get_session_data_and_clicks_per_session()\n",
    "\n",
    "click_models = {'random': rcm,\n",
    "                'position-based': pbm}\n",
    "\n",
    "interleavings = {'probabilistic': ProbabilisticInterleaving,\n",
    "                 'team-draft': TeamDraftInterleaving}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_from_bucket_generator(bucket, group_size=100):\n",
    "    ''' Bootstrap sampling '''\n",
    "    relevances = [i for rel in bucket['relevances'] for i in bucket['relevances'][rel]['rankings']]\n",
    "    indices = np.arange(len(relevances))\n",
    "    sampled_indices = np.random.choice(indices, size=group_size, replace=True)\n",
    "    \n",
    "    samples = [relevances[i] for i in sampled_indices]\n",
    "    return samples\n",
    "\n",
    "\n",
    "def simulate_interleaving_experiment(buckets, interleaving_factory, click_model, n_simulations):\n",
    "    '''\n",
    "    Simulates click experiments and stores win results in buckets[id][wins]\n",
    "    '''\n",
    "    for bucket in buckets.values():\n",
    "        wins = {'P': 0, 'E': 0, 'T': 0}\n",
    "        bucket['wins'] = wins\n",
    "        samples = sample_from_bucket_generator(bucket)\n",
    "        for ranking in samples:\n",
    "            for j in range(n_simulations):\n",
    "                prod_ranking, exp_ranking = ranking.P, ranking.E\n",
    "                interleaved, prod_docs, exp_docs = interleaving_factory(\n",
    "                    prod_ranking, exp_ranking).run()\n",
    "\n",
    "                relevances = []\n",
    "                for doc in interleaved:\n",
    "                    if doc in prod_docs:\n",
    "                        docs = prod_docs\n",
    "                        rel = ranking.rel_P\n",
    "                    else:\n",
    "                        docs = exp_docs\n",
    "                        rel = ranking.rel_E\n",
    "\n",
    "                    i = docs.index(doc)\n",
    "                    relevances.append(rel[i])\n",
    "\n",
    "                clicks = click_model.simulate_clicks(interleaved, relevances)\n",
    "\n",
    "                if DEBUG:\n",
    "                    print('relevances:', relevances)\n",
    "                    print(ranking)\n",
    "                    print('Rankings:', prod_ranking, exp_ranking)\n",
    "                    print('interleaved:', interleaved)\n",
    "                    print('prod_docs', prod_docs)\n",
    "                    print('exp_docs', exp_docs)\n",
    "                    print('clicks', [interleaved[c] for c in clicks])\n",
    "\n",
    "                winner = assign_winner(clicks, exp_docs, prod_docs, interleaved)\n",
    "                if winner == 'E':\n",
    "                    wins['E'] += 1\n",
    "                elif winner == 'P':\n",
    "                    wins['P'] += 1\n",
    "                else:\n",
    "                    wins['T'] += 1\n",
    "\n",
    "\n",
    "def assign_winner(clicks, exp_docs, prod_docs, interleaved):\n",
    "    ''' Assign a win to the system with the most clicks '''\n",
    "    click_count_E = 0\n",
    "    click_count_P = 0\n",
    "\n",
    "    for click_idx in clicks:\n",
    "        clicked_doc = interleaved[click_idx]\n",
    "        if clicked_doc in exp_docs:\n",
    "            click_count_E += 1\n",
    "        elif clicked_doc in prod_docs:\n",
    "            click_count_P += 1\n",
    "\n",
    "    if click_count_E > click_count_P:\n",
    "        return 'E'\n",
    "    elif click_count_P > click_count_E:\n",
    "        return 'P'\n",
    "    else:\n",
    "        return 'T'\n",
    "\n",
    "\n",
    "def run_click_experiments(k):\n",
    "    ''' Run the experiments for each click model and interleaving method '''\n",
    "    results = defaultdict(dict)\n",
    "    for click_model_name, click_model in click_models.items():\n",
    "        for interleaving_name, interleaving_factory in interleavings.items():\n",
    "            buckets = get_buckets()\n",
    "            simulate_interleaving_experiment(\n",
    "                buckets, interleaving_factory, click_model, n_simulations=k)\n",
    "\n",
    "            results[interleaving_name][click_model_name] = \\\n",
    "                {b_id: bucket['wins'] for b_id, bucket in buckets.items()}\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test for getting the number of wins after running the click experiment\n",
    "results = run_click_experiments(1)\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample size - Power analysis\n",
    "\n",
    "In this section, we determine the sample size for multiple iterations in order to get a distribution according to the $\\alpha = 0.05$ and $\\beta = 0.1$.\n",
    "* We have discarded ties for calculating the proportions p_1, by defining it as $p_1 = \\frac{wins_E}{wins_E + wins_P}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sample_size(p_1, p_0=0.5, alpha=0.05, beta=0.1):\n",
    "    '''\n",
    "        we set p0 = 0.5 to reflect that each system wins 50% of times.\n",
    "        p1 is the proportion of times E wins over P out of k simulations.\n",
    "        α and β specify the level of significance.\n",
    "            We set α = 0.05 and β = 0.1.\n",
    "        δ is |p1 − p0|\n",
    "        z is the standard normal distribution\n",
    "\n",
    "        Finally, using the continuity correction the minimum sample\n",
    "        size is determined as:\n",
    "\n",
    "        Source:\n",
    "        @inproceedings{azarbonyad2016power,\n",
    "          title={Power Analysis for Interleaving Experiments by means of Offline Evaluation},\n",
    "          author={Azarbonyad, Hosein and Kanoulas, Evangelos},\n",
    "          booktitle={Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},\n",
    "          pages={87--90},\n",
    "          year={2016},\n",
    "          organization={ACM}\n",
    "        }\n",
    "\n",
    "        See also:\n",
    "        https://en.wikipedia.org/wiki/Notation_in_probability_and_statistics#Critical_values\n",
    "    '''\n",
    "\n",
    "    delta = abs(p_1 - p_0)\n",
    "    z_alpha = norm.ppf(1-alpha)\n",
    "    z_beta = norm.ppf(1-beta)\n",
    "    \n",
    "    N = (z_alpha * sqrt(p_0*(1-p_0)) + z_beta * sqrt(p_1*(1-p_1)))\n",
    "\n",
    "    if N == 0.0 or (p_1 - p_0) == 0.0:\n",
    "        raise ValueError()\n",
    "\n",
    "    N = N / delta\n",
    "    N = N**2\n",
    "    N = N + 1/delta\n",
    "    return ceil(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nested_defaultdict(depth, inner_factory=list):\n",
    "    if depth <= 1:\n",
    "        return defaultdict(list)\n",
    "    else:\n",
    "        return defaultdict(lambda: nested_defaultdict(depth-1))\n",
    "    \n",
    "def determine_sample_sizes(n, k):\n",
    "    table = nested_defaultdict(4)\n",
    "    for i in range(n):\n",
    "        print('iteration:', i)\n",
    "        results = run_click_experiments(k)\n",
    "\n",
    "        for interleaving, clicks in results.items():\n",
    "            for click_model, bucket in clicks.items():\n",
    "                for b_id, wins in bucket.items():\n",
    "                    if (wins['E'] + wins['P']) > 0:\n",
    "                        p_1 = wins['E'] / (wins['E'] + wins['P'])     # proportion of wins\n",
    "                        try:\n",
    "                            N = get_sample_size(p_1)\n",
    "                            table[interleaving][click_model][b_id]['p_1'].append(p_1)\n",
    "                            table[interleaving][click_model][b_id]['N'].append(N)\n",
    "                        except ValueError:\n",
    "                            pass\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def get_final_table(sample_sizes):\n",
    "    final_table = nested_defaultdict(4)\n",
    "    plot_data = nested_defaultdict(4)\n",
    "    for interleaving, clicks in sample_sizes.items():\n",
    "        for click_model, data in clicks.items():\n",
    "            for b_id, bucket in data.items():\n",
    "                final_table[interleaving][click_model][b_id]['min N'] = np.min(\n",
    "                    bucket['N'])\n",
    "                final_table[interleaving][click_model][b_id]['median N'] = np.median(\n",
    "                    bucket['N'])\n",
    "                final_table[interleaving][click_model][b_id]['max N'] = np.max(\n",
    "                    bucket['N'])\n",
    "                plot_data[interleaving][click_model][b_id] = bucket['N']\n",
    "    return final_table, plot_data\n",
    "\n",
    "\n",
    "def plot_results(data):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    for interleaving_model, ax_rows in zip(data.items(), axes):\n",
    "        interleaving_name, inter_data = interleaving_model\n",
    "        for click_model, axis in zip(inter_data.items(), ax_rows):\n",
    "            click_name, data = click_model\n",
    "            x = []\n",
    "            y = []\n",
    "            for idx, d in data.items():\n",
    "                x.append(0.1 * idx + 0.05)\n",
    "                y.append(np.median(d))\n",
    "            axis.bar(x, height=y, width=0.05)\n",
    "            axis.grid()\n",
    "            axis.set_xlabel(r'$\\Delta ERR$')\n",
    "            axis.set_title(\"{}, {}\".format(interleaving_name, click_name))\n",
    "    fig.suptitle(\"Distribution of N over $\\Delta ERR$\")\n",
    "    plt.show()\n",
    "    \n",
    "def print_final_table(final_table):\n",
    "    for interleaving in final_table.keys():\n",
    "        for click_model in final_table[interleaving].keys():\n",
    "            print('\\n')\n",
    "            print('Interleaving Method: {} - Click Model: {}'.format(interleaving, click_model))\n",
    "            print('----------| Min |---| Median |---| Max |')\n",
    "            for b_id, bucket in final_table[interleaving][click_model].items():\n",
    "                print('bucket{}---| {} |---| {} |---| {} |'.format(b_id+1, bucket['min N'],\n",
    "                        bucket['median N'], bucket['max N']))\n",
    "                print('-'*44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run experiments\n",
    "#k = 50, WHY\n",
    "#n = \n",
    "sample_sizes = determine_sample_sizes(n=20, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_table, plot_data = get_final_table(sample_sizes)\n",
    "plot_results(plot_data)\n",
    "print_final_table(final_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline\n",
    "#### ERR\n",
    "The ERR is a measure based on the cascade model, meaning that the position of the document is relevant to calculate this metric. \n",
    "Of course this is not an optimal measure, since the clicking behavior of a user depends on many other factors. Other measures could have also been evaluated to provide more complete analysis for this experiment.\n",
    "\n",
    "### Online\n",
    "\n",
    "### Tables comparison:\n",
    "\n",
    "##### RCM and PBM:\n",
    "\n",
    "Regardless of the interleaving method, the random click model acts randomly during the online experiments, contrary to the position-based model. The RCM does not take relevances or order into account and therefore the two systems can perform very similarly, hence to justify systems that have similar performances based on clicks, we need a big sample size. This corresponds to the table results: the random click model's tables have much higher required sample sizes compared to the position-based model's tables. This likely comes from the delta calculation in the power analysis formula, where a smaller delta will mean a larger required sample size. \n",
    "The PBM does take the relevances into account, which directly affects the click probability (through $\\alpha$). Since we have already filtered pairs of relevances with positive $\\Delta$ERR-s, the click probability is higher for System E. Unlike the random click model, in the PBM as we increase the $\\Delta$ERR, the required samples get smaller. Intuitively, this makes sense, as a higher $\\Delta$ERR means that there was a bigger difference in the systems already in the offline experiments.\n",
    "\n",
    "\n",
    "##### Team-Draft and Probabilistic interleaving:\n",
    "\n",
    "The team draft interleaving method is biased in the sense that when we combine the documents from both systems and run this multiple times we will get very similar results, i.e. skewed. However, in the Probabilistic interleaving, we take into account all possible combinations. When we run this multiple times, a position bias disappears since all documents have a probability of being selected. \n",
    "\n",
    "The tables show that the robabilistic-interleaving method has higher required samples sizes than the Team-draft. This relates to our point mentioned above that the position bias disappears, and this has an upscale effect on the required sample size since the proportion of wins for system E becomes smaller.\n",
    "\n",
    "\n",
    "### Downsides from the experiment\n",
    "Apart from the downsides mentioned above, there are a few more assumptions that were made during designing and running this experiment that do not necessarily reflect a practical use-case. \n",
    "* We have only used 2 relevance labels and a ranking of 3. This is a very simplistic approach.\n",
    "* We assumed that all possible ranking pairs exist. This may not be true in a real system.\n",
    "* We removed all pairs with a negative relevance. Some pairs might perform better online, namely the ones close to a 0 $\\Delta$ERR.\n",
    "\n",
    "\n",
    "### Future improvements for the experiment\n",
    "The clicks model that we have used are quite simplistic and do not really model real user behavior. Different, more advanced models could be implemented e.g. Cascade models.\n",
    "\n",
    "More evaluation measures could be used e.g. DCG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "772px",
    "left": "113px",
    "top": "146.281px",
    "width": "346.359px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
